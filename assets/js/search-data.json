{
  
    
        "post0": {
            "title": "tipToe Dive into Machine Learning",
            "content": "import pandas as pd # import numpy as np # import matplotlib.pyplot as plt !pip install statsmodels . Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.10.2) Requirement already satisfied: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.18.5) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.1) Requirement already satisfied: scipy&gt;=0.18 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.4.1) Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.1.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy&gt;=0.4.0-&gt;statsmodels) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2.8.1) . Supervised Learning - Linear Regression Approach . Portland Housing Price Prediction Dataset . input variables, $x^{(i)}$: Living area ($x_1^{(i)}$), #bedrooms ($x_2^{(i)}$). output variable, $y^{(i)}$: Price ($y^{(i)}$). We only have one output variable training set: $ {x^{(i)},y^{(i)};i=1,...,n }$, where $n=47$ . len(ptlnd) 47 . ptlnd = pd.read_csv(&#39;ex1data2.txt&#39;,header=None) # ptval = ptlnd.iloc[:,2].values # ptindex = ptlnd.iloc[:,0].values . . statsmodels . installing statsmodels and its dependencies patsy is a Python library for describing statistical models and building Design Matrices . import statsmodels.api as sm from patsy import dmatrices . ptlnd.columns . Int64Index([0, 1, 2], dtype=&#39;int64&#39;) . renaming columns . ptlnd.rename(columns={0:&#39;Living_area&#39;,1:&#39;bedrooms&#39;,2:&#39;Price&#39;},inplace=True) . ptlnd.columns . Index([&#39;Living_area&#39;, &#39;bedrooms&#39;, &#39;Price&#39;], dtype=&#39;object&#39;) . create two design matrices to fit estimated model (OLS) . $y$ : matrix of endogenous variables (dependent), $N times 1$ column data ~ Price | $X$ : matrix exogenous varaibles (independent), $N times 3$ column data ~ y-Intercept, Living Area, #bedrooms | . y, X = dmatrices(&#39;Price ~ Living_area + bedrooms&#39;,data=ptlnd,return_type=&#39;dataframe&#39;) . Design Matrices . y.head() . Price . 0 399900.0 | . 1 329900.0 | . 2 369000.0 | . 3 232000.0 | . 4 539900.0 | . X.head() . Intercept Living_area bedrooms . 0 1.0 | 2104.0 | 3.0 | . 1 1.0 | 1600.0 | 3.0 | . 2 1.0 | 2400.0 | 3.0 | . 3 1.0 | 1416.0 | 2.0 | . 4 1.0 | 3000.0 | 4.0 | . Model fit and summary . Describe, fit and summarize model using Ordinary Least Squares (OLS) | . mod = sm.OLS(y,X) # fit model res = mod.fit() # Summarize model print(res.summary()) . OLS Regression Results ============================================================================== Dep. Variable: Price R-squared: 0.733 Model: OLS Adj. R-squared: 0.721 Method: Least Squares F-statistic: 60.38 Date: Thu, 08 Oct 2020 Prob (F-statistic): 2.43e-13 Time: 19:56:42 Log-Likelihood: -586.77 No. Observations: 47 AIC: 1180. Df Residuals: 44 BIC: 1185. Df Model: 2 Covariance Type: nonrobust =============================================================================== coef std err t P&gt;|t| [0.025 0.975] - Intercept 8.96e+04 4.18e+04 2.145 0.037 5421.208 1.74e+05 Living_area 139.2107 14.795 9.409 0.000 109.393 169.028 bedrooms -8738.0191 1.55e+04 -0.566 0.575 -3.99e+04 2.24e+04 ============================================================================== Omnibus: 4.083 Durbin-Watson: 1.826 Prob(Omnibus): 0.130 Jarque-Bera (JB): 2.977 Skew: 0.567 Prob(JB): 0.226 Kurtosis: 3.484 Cond. No. 9.71e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 9.71e+03. This might indicate that there are strong multicollinearity or other numerical problems. . Extracting parameter estimates and $r^2$ . res.params . Intercept 89597.909543 Living_area 139.210674 bedrooms -8738.019112 dtype: float64 . res.rsquared . 0.7329450180289142 . Applying Rainbow Test for Linearity . The basic idea of the Rainbow test is that even if the true relationship is non-linear, a good linear fit can be achieved on a subsample in the &quot;middle&quot; of the data | . sm.stats.linear_rainbow(res) . (0.49348806706947385, 0.9499682609135575) . Draw a plot of partial regression for a set of regressors . sm.graphics.plot_partregress(&#39;Price&#39;,&#39;Living_area&#39;,[&#39;bedrooms&#39;],data=ptlnd,obs_labels=False) . sklearn . ptlnd . Living_area bedrooms Price . 0 2104 | 3 | 399900 | . 1 1600 | 3 | 329900 | . 2 2400 | 3 | 369000 | . 3 1416 | 2 | 232000 | . 4 3000 | 4 | 539900 | . 5 1985 | 4 | 299900 | . 6 1534 | 3 | 314900 | . 7 1427 | 3 | 198999 | . 8 1380 | 3 | 212000 | . 9 1494 | 3 | 242500 | . 10 1940 | 4 | 239999 | . 11 2000 | 3 | 347000 | . 12 1890 | 3 | 329999 | . 13 4478 | 5 | 699900 | . 14 1268 | 3 | 259900 | . 15 2300 | 4 | 449900 | . 16 1320 | 2 | 299900 | . 17 1236 | 3 | 199900 | . 18 2609 | 4 | 499998 | . 19 3031 | 4 | 599000 | . 20 1767 | 3 | 252900 | . 21 1888 | 2 | 255000 | . 22 1604 | 3 | 242900 | . 23 1962 | 4 | 259900 | . 24 3890 | 3 | 573900 | . 25 1100 | 3 | 249900 | . 26 1458 | 3 | 464500 | . 27 2526 | 3 | 469000 | . 28 2200 | 3 | 475000 | . 29 2637 | 3 | 299900 | . 30 1839 | 2 | 349900 | . 31 1000 | 1 | 169900 | . 32 2040 | 4 | 314900 | . 33 3137 | 3 | 579900 | . 34 1811 | 4 | 285900 | . 35 1437 | 3 | 249900 | . 36 1239 | 3 | 229900 | . 37 2132 | 4 | 345000 | . 38 4215 | 4 | 549000 | . 39 2162 | 4 | 287000 | . 40 1664 | 2 | 368500 | . 41 2238 | 3 | 329900 | . 42 2567 | 4 | 314000 | . 43 1200 | 3 | 299000 | . 44 852 | 2 | 179900 | . 45 1852 | 4 | 299900 | . 46 1203 | 3 | 239500 | . Goal: given a dataset $ {x^{(i)},y^{(i)};i=1,...,n }$, how can we produce a function $h$ such that $h(x) approx y$ assumming $y$ is a linear function of $x$: $h_ theta(x)= theta_0+ theta_1x_1+ theta_2x_2$: where $x_0 = 1$ (y-intercept) . from sklearn import linear_model reg = linear_model.LinearRegression() reg.fit(ptlnd.iloc[:,0:2],ptlnd.iloc[:,2]) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . reg.coef_ . array([ 139.21067402, -8738.01911233]) . import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import mean_squared_error,r2_score ptlnd_x_train = ptlnd.iloc[:,0:2][:-16] ptlnd_x_test = ptlnd.iloc[:,0:2][-16:] ptlnd_y_train =ptlnd.iloc[:,2][:-16] ptlnd_y_test =ptlnd.iloc[:,2][-16:] regr = linear_model.LinearRegression() regr.fit(ptlnd_x_train,ptlnd_y_train) ptlnd_y_pred = regr.predict(ptlnd_x_test) plt.scatter(ptlnd_x_test.iloc[:,0],ptlnd_y_test,color=&#39;black&#39;) plt.plot(ptlnd_x_test.iloc[:,0],ptlnd_y_pred,color=&#39;blue&#39;,linewidth=3) . [&lt;matplotlib.lines.Line2D at 0x7f77f548b240&gt;] . regr.coef_ . array([ 141.97792925, 6371.72067488]) . r2_score(ptlnd_y_test,ptlnd_y_pred) . 0.6790116553302931 . using cs229 dataset - portland housing . print(f&quot;Average rent is ${ptlnd[&#39;Price&#39;].mean():.0f}&quot;) . Average rent is $340413 . bybed = ptlnd.groupby([&#39;bedrooms&#39;]).mean().reset_index() print(bybed[[&#39;bedrooms&#39;,&#39;Price&#39;]]) . bedrooms Price 0 1 169900.000000 1 2 280866.666667 2 3 326403.920000 3 4 377449.785714 4 5 699900.000000 . import matplotlib.pyplot as plt bybed.plot.line(&#39;bedrooms&#39;,&#39;Price&#39;,style=&#39;-o&#39;) plt.show() . X,y = ptlnd[[&#39;Living_area&#39;,&#39;bedrooms&#39;]],ptlnd[&#39;Price&#39;] . print(type(X), type(y)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(n_estimators=10) rf.fit(X,y) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . unknown_xxx = [[1427,1],[5427,3],[2587,5]] p = [] for i in range(len(unknown_xxx)): predicted_yyy = rf.predict([unknown_xxx[i]]) p.extend(predicted_yyy) print(p) . [267410.0, 619920.0, 440559.6] . from sklearn.metrics import mean_absolute_error predictions = rf.predict(X) e = mean_absolute_error(y,predictions) ep = e*100/y.mean() print(f&quot;${e:.0f} average error; {ep:.2f}% error&quot;) . $28151 average error; 8.27% error . from sklearn.model_selection import train_test_split X,y = ptlnd[[&#39;Living_area&#39;,&#39;bedrooms&#39;]],ptlnd[&#39;Price&#39;] X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2) rf = RandomForestRegressor(n_estimators=10) rf.fit(X_train,y_train) validation_e = mean_absolute_error(y_test,rf.predict(X_test)) print(f&quot;${validation_e:.0f} average error; {validation_e*100/y.mean()}% error&quot;) . $71396 average error; 20.97331224086911% error . rf = RandomForestRegressor(n_estimators=100) rf.fit(X_train,y_train) e = mean_absolute_error(y_test,rf.predict(X_test)) print(f&quot;${e:.0f} average error; {e*100/y.mean():.2f}% error&quot;) . $70147 average error; 20.61% error . !pip install rfpimp from rfpimp import * rf = RandomForestRegressor(n_estimators=100) rf.fit(X_train,y_train) I = importances(rf,X_test,y_test) I . Collecting rfpimp Downloading https://files.pythonhosted.org/packages/36/05/ce8f1d3a035a4ddda3c888af945908e9f61cbec32f73148fc8e788a8632a/rfpimp-1.3.5.tar.gz Collecting stratx&gt;=0.2 Downloading https://files.pythonhosted.org/packages/93/44/754e5f4c5fcb68968fe0fadffcc30f8c2194a855a4329dd870007921506c/stratx-0.5.tar.gz (160kB) |████████████████████████████████| 163kB 10.2MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rfpimp) (1.18.5) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from rfpimp) (1.1.2) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from rfpimp) (0.22.2.post1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from rfpimp) (3.2.2) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stratx&gt;=0.2-&gt;rfpimp) (1.4.1) Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from stratx&gt;=0.2-&gt;rfpimp) (0.48.0) Collecting colour Downloading https://files.pythonhosted.org/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;rfpimp) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;rfpimp) (2018.9) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;rfpimp) (0.16.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (2.4.7) Requirement already satisfied: llvmlite&lt;0.32.0,&gt;=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba-&gt;stratx&gt;=0.2-&gt;rfpimp) (0.31.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba-&gt;stratx&gt;=0.2-&gt;rfpimp) (50.3.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;rfpimp) (1.15.0) Building wheels for collected packages: rfpimp, stratx Building wheel for rfpimp (setup.py) ... done Created wheel for rfpimp: filename=rfpimp-1.3.5-cp36-none-any.whl size=10261 sha256=14a518caf23937ad89df86f9a79e8626e6852c596ab5b56208c73e7cfef8d924 Stored in directory: /root/.cache/pip/wheels/2a/c8/bf/80f91224be00870f84a85aa1782cf6cd62c1289a173b66b8e9 Building wheel for stratx (setup.py) ... done Created wheel for stratx: filename=stratx-0.5-cp36-none-any.whl size=35033 sha256=496e9d50e5f7b46ececab86f5eb18fb7dfeb6f8e6a76f7877a058d2d531b7a41 Stored in directory: /root/.cache/pip/wheels/6e/28/de/a84563cfe761afd6c628e595590455412e26a6c4d587289d7b Successfully built rfpimp stratx Installing collected packages: colour, stratx, rfpimp Successfully installed colour-0.1.5 rfpimp-1.3.5 stratx-0.5 . /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API. warnings.warn(message, FutureWarning) . Importance . Feature . Living_area 1.750672 | . bedrooms -0.085395 | . plot_importances(I,color=&#39;#4575b4&#39;,vscale=1.8) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; rf.score(X_test,y_test) . 0.2243195168360498 . A model is a combination of data structure, algorithm, and mathematics that captures the relationship described by a collection of (feature vector, target) pairs. The model records a condensation of the training data in its data structure, which can be anything from the unaltered training set (nearest neighbor model) to a set of decision trees (random forest model) to a handful of weights (linear model). This data structure comprises the parameters of the model and the parameters are computed from the training data. . Decision Trees . !pip install rfpimp . Requirement already satisfied: rfpimp in /usr/local/lib/python3.6/dist-packages (1.3.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from rfpimp) (0.22.2.post1) Requirement already satisfied: stratx&gt;=0.2 in /usr/local/lib/python3.6/dist-packages (from rfpimp) (0.5) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rfpimp) (1.18.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from rfpimp) (3.2.2) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from rfpimp) (1.1.2) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;rfpimp) (0.16.0) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;rfpimp) (1.4.1) Requirement already satisfied: colour in /usr/local/lib/python3.6/dist-packages (from stratx&gt;=0.2-&gt;rfpimp) (0.1.5) Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from stratx&gt;=0.2-&gt;rfpimp) (0.48.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (2.4.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;rfpimp) (1.2.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;rfpimp) (2018.9) Requirement already satisfied: llvmlite&lt;0.32.0,&gt;=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba-&gt;stratx&gt;=0.2-&gt;rfpimp) (0.31.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba-&gt;stratx&gt;=0.2-&gt;rfpimp) (50.3.0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;rfpimp) (1.15.0) . !python prep-rent.py . Traceback (most recent call last): File &#34;prep-rent.py&#34;, line 17, in &lt;module&gt; df = pd.read_json(&#39;train.json&#39;) File &#34;/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py&#34;, line 199, in wrapper return func(*args, **kwargs) File &#34;/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py&#34;, line 296, in wrapper return func(*args, **kwargs) File &#34;/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py&#34;, line 618, in read_json result = json_reader.read() File &#34;/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py&#34;, line 755, in read obj = self._get_object_parser(self.data) File &#34;/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py&#34;, line 777, in _get_object_parser obj = FrameParser(json, **kwargs).parse() File &#34;/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py&#34;, line 886, in parse self._parse_no_numpy() File &#34;/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py&#34;, line 1119, in _parse_no_numpy loads(json, precise_float=self.precise_float), dtype=None ValueError: Unmatched &#39;&#39;&#34;&#39; when when decoding &#39;string&#39; . import pandas as pd rent = pd.read_csv(&#39;rent-ideal.csv&#39;) . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-45-39171f49fb23&gt; in &lt;module&gt;() 1 import pandas as pd -&gt; 2 rent = pd.read_csv(&#39;rent-ideal.csv&#39;) /usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 684 ) 685 --&gt; 686 return _read(filepath_or_buffer, kwds) 687 688 /usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds) 450 451 # Create the parser. --&gt; 452 parser = TextFileReader(fp_or_buf, **kwds) 453 454 if chunksize or iterator: /usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds) 934 self.options[&#34;has_index_names&#34;] = kwds[&#34;has_index_names&#34;] 935 --&gt; 936 self._make_engine(self.engine) 937 938 def close(self): /usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in _make_engine(self, engine) 1166 def _make_engine(self, engine=&#34;c&#34;): 1167 if engine == &#34;c&#34;: -&gt; 1168 self._engine = CParserWrapper(self.f, **self.options) 1169 else: 1170 if engine == &#34;python&#34;: /usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py in __init__(self, src, **kwds) 1996 kwds[&#34;usecols&#34;] = self.usecols 1997 -&gt; 1998 self._reader = parsers.TextReader(src, **kwds) 1999 self.unnamed_cols = self._reader.unnamed_cols 2000 pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source() FileNotFoundError: [Errno 2] No such file or directory: &#39;rent-ideal.csv&#39; . print(f&quot;Average rent is ${rent[&#39;price&#39;].mean():.0f}&quot;) . Average rent is $3438 . data aggregation . bybaths = rent.groupby([&#39;bathrooms&#39;]).mean().reset_index() print(bybaths[[&#39;bathrooms&#39;,&#39;price&#39;]]) . bathrooms price 0 0.0 3144.870000 1 1.0 3027.007118 2 1.5 4226.336449 3 2.0 5278.595739 4 2.5 6869.047368 5 3.0 6897.974576 6 3.5 7635.357143 7 4.0 7422.888889 8 4.5 2050.000000 9 10.0 3600.000000 . import matplotlib.pyplot as plt import numpy as np bybaths.plot.line(&#39;bathrooms&#39;,&#39;price&#39;,style=&#39;-o&#39;) plt.show() . X,y = rent[[&#39;bedrooms&#39;,&#39;bathrooms&#39;,&#39;latitude&#39;,&#39;longitude&#39;]], rent[&#39;price&#39;] . print(type(X),type(y)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; . sklearn . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(n_estimators=10) rf.fit(X,y) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . using a dummy exmaple . unknown_x =[2,1,40.7957,-73.97] . predicted_y = rf.predict([unknown_x]) print(predicted_y) . [4216.71680574] . using a list of dummy examples . unknown_xx =[ (2,1,40.7957,-73.97),(3,1,40.7957,-73.97), (2,500,40.7957,-73.97),(4,3,56,-1.4) ] . p = [] for i in range(len(unknown_xx)): predicted_yy = rf.predict([unknown_xx[i]]) p.extend(predicted_yy) print(p) . [4216.716805738545, 4722.982142857143, 6174.8, 2660.0] . testing (best fit) . from sklearn.metrics import mean_absolute_error predictions = rf.predict(X) e = mean_absolute_error(y,predictions) ep = e*100.0/y.mean() # training error print(f&quot;${e:.0f} average error; {ep:.2f}% error&quot;) . $189 average error; 5.51% error . &#39;dimensionality reduction&#39; . X, y = rent[[&#39;latitude&#39;,&#39;longitude&#39;]], rent[&#39;price&#39;] rf = RandomForestRegressor(n_estimators=100) rf.fit(X,y) location_e = mean_absolute_error(y,rf.predict(X)) location_ep = location_e*100.0/y.mean() print(f&quot;${location_e:.0f} average error; {location_ep:.2f}% error&quot;) . $519 average error; 15.09% error . reduction = ep*100/location_ep print(f&quot;{reduction:.2f}% reduction in accuracy when bedrooms and bathrooms features are dropped&quot;) . 36.51% reduction in accuracy when bedrooms and bathrooms features are dropped . generalization . from sklearn.model_selection import train_test_split X, y = rent[[&#39;bedrooms&#39;,&#39;bathrooms&#39;,&#39;latitude&#39;,&#39;longitude&#39;]], rent[&#39;price&#39;] # 20% of data goes into test set, 80% into training set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2) rf = RandomForestRegressor(n_estimators=10) rf.fit(X_train,y_train) validation_e = mean_absolute_error(y_test,rf.predict(X_test)) print(f&quot;${validation_e:.0f} average error; {validation_e*100.0/y.mean():.2f}% error&quot;) . $297 average error; 8.64% error . rf = RandomForestRegressor(n_estimators=100) rf.fit(X_train,y_train) e = mean_absolute_error(y_test,rf.predict(X_test)) print(f&quot;${e:.0f} average error; {e*100.0/y.mean():.2f}% error&quot;) . $290 average error; 8.43% error . feature importance . from rfpimp import * rf = RandomForestRegressor(n_estimators=100) rf.fit(X_train,y_train) I = importances(rf,X_test,y_test) I . Importance . Feature . bedrooms 0.523889 | . longitude 0.489262 | . latitude 0.451691 | . bathrooms 0.444979 | . feature importance graphs . plot_importances(I,color=&#39;#4575b4&#39;,vscale=1.8) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; I =importances(rf,X_test,y_test, features=[&#39;bedrooms&#39;,&#39;bedrooms&#39;,[&#39;latitude&#39;,&#39;longitude&#39;]]) plot_importances(I,color=&#39;#4575b4&#39;,vscale=1.8) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Classification . Predicting Breast Cancer . from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() X = cancer.data y = cancer.target df = pd.DataFrame(X,columns=cancer.feature_names) . features = [&#39;radius error&#39;,&#39;texture error&#39;,&#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;worst texture&#39;,&#39;worst smoothness&#39;, &#39;worst symmetry&#39;] df = df[features] print(&#39;target[0:30] = &#39;,y[0:30]) df.head() . target[0:30] = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0] . radius error texture error concave points error symmetry error worst texture worst smoothness worst symmetry . 0 1.0950 | 0.9053 | 0.01587 | 0.03003 | 17.33 | 0.1622 | 0.4601 | . 1 0.5435 | 0.7339 | 0.01340 | 0.01389 | 23.41 | 0.1238 | 0.2750 | . 2 0.7456 | 0.7869 | 0.02058 | 0.02250 | 25.53 | 0.1444 | 0.3613 | . 3 0.4956 | 1.1560 | 0.01867 | 0.05963 | 26.50 | 0.2098 | 0.6638 | . 4 0.7572 | 0.7813 | 0.01885 | 0.01756 | 16.67 | 0.1374 | 0.2364 | . X_train, X_test, y_train, y_test = train_test_split(df,y,test_size=.15) . from sklearn.ensemble import RandomForestClassifier cl = RandomForestClassifier(n_estimators=300) cl.fit(X_train,y_train) validation_e = cl.score(X_test,y_test) print(f&quot;{validation_e*100:.2f}% correct&quot;) . 97.67% correct . !git clone https://github.com/jadeyee/r2d3-part-1-data.git . Cloning into &#39;r2d3-part-1-data&#39;... remote: Enumerating objects: 10, done. remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 10 Unpacking objects: 100% (10/10), done. . import pandas as pd . !pip install gspread-pandas . Collecting gspread-pandas Downloading https://files.pythonhosted.org/packages/bf/18/9e95b3aaaa6de6da317a35d98d23a129529a380d443d53b05bd07036792d/gspread_pandas-2.2.3-py2.py3-none-any.whl Requirement already satisfied: pandas&gt;=0.20.0 in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (1.1.2) Requirement already satisfied: gspread&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (3.0.1) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (0.16.0) Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (4.4.2) Requirement already satisfied: google-auth in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (1.17.2) Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gspread-pandas) (0.4.1) Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.20.0-&gt;gspread-pandas) (1.18.5) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.20.0-&gt;gspread-pandas) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.20.0-&gt;gspread-pandas) (2.8.1) Requirement already satisfied: requests&gt;=2.2.1 in /usr/local/lib/python3.6/dist-packages (from gspread&gt;=3.0.0-&gt;gspread-pandas) (2.23.0) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-&gt;gspread-pandas) (1.15.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-&gt;gspread-pandas) (4.1.1) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from google-auth-&gt;gspread-pandas) (4.6) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth-&gt;gspread-pandas) (0.2.8) Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-&gt;gspread-pandas) (50.3.0) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib-&gt;gspread-pandas) (1.3.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.2.1-&gt;gspread&gt;=3.0.0-&gt;gspread-pandas) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.2.1-&gt;gspread&gt;=3.0.0-&gt;gspread-pandas) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.2.1-&gt;gspread&gt;=3.0.0-&gt;gspread-pandas) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.2.1-&gt;gspread&gt;=3.0.0-&gt;gspread-pandas) (2020.6.20) Requirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34;-&gt;google-auth-&gt;gspread-pandas) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib-&gt;gspread-pandas) (3.1.0) Installing collected packages: gspread-pandas Successfully installed gspread-pandas-2.2.3 . from gspread_pandas import Spread, Client . !pwd . /content . Predicting BC . from sklearn.datasets import load_breast_cancer import pandas as pd cancer = load_breast_cancer() X = cancer.data y = cancer.target df = pd.DataFrame(X,columns=cancer.feature_names) type(cancer) . sklearn.utils.Bunch . df . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . 0 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.30010 | 0.14710 | 0.2419 | 0.07871 | 1.0950 | 0.9053 | 8.589 | 153.40 | 0.006399 | 0.04904 | 0.05373 | 0.01587 | 0.03003 | 0.006193 | 25.380 | 17.33 | 184.60 | 2019.0 | 0.16220 | 0.66560 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.08690 | 0.07017 | 0.1812 | 0.05667 | 0.5435 | 0.7339 | 3.398 | 74.08 | 0.005225 | 0.01308 | 0.01860 | 0.01340 | 0.01389 | 0.003532 | 24.990 | 23.41 | 158.80 | 1956.0 | 0.12380 | 0.18660 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.19740 | 0.12790 | 0.2069 | 0.05999 | 0.7456 | 0.7869 | 4.585 | 94.03 | 0.006150 | 0.04006 | 0.03832 | 0.02058 | 0.02250 | 0.004571 | 23.570 | 25.53 | 152.50 | 1709.0 | 0.14440 | 0.42450 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.24140 | 0.10520 | 0.2597 | 0.09744 | 0.4956 | 1.1560 | 3.445 | 27.23 | 0.009110 | 0.07458 | 0.05661 | 0.01867 | 0.05963 | 0.009208 | 14.910 | 26.50 | 98.87 | 567.7 | 0.20980 | 0.86630 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | . 4 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.19800 | 0.10430 | 0.1809 | 0.05883 | 0.7572 | 0.7813 | 5.438 | 94.44 | 0.011490 | 0.02461 | 0.05688 | 0.01885 | 0.01756 | 0.005115 | 22.540 | 16.67 | 152.20 | 1575.0 | 0.13740 | 0.20500 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 564 21.56 | 22.39 | 142.00 | 1479.0 | 0.11100 | 0.11590 | 0.24390 | 0.13890 | 0.1726 | 0.05623 | 1.1760 | 1.2560 | 7.673 | 158.70 | 0.010300 | 0.02891 | 0.05198 | 0.02454 | 0.01114 | 0.004239 | 25.450 | 26.40 | 166.10 | 2027.0 | 0.14100 | 0.21130 | 0.4107 | 0.2216 | 0.2060 | 0.07115 | . 565 20.13 | 28.25 | 131.20 | 1261.0 | 0.09780 | 0.10340 | 0.14400 | 0.09791 | 0.1752 | 0.05533 | 0.7655 | 2.4630 | 5.203 | 99.04 | 0.005769 | 0.02423 | 0.03950 | 0.01678 | 0.01898 | 0.002498 | 23.690 | 38.25 | 155.00 | 1731.0 | 0.11660 | 0.19220 | 0.3215 | 0.1628 | 0.2572 | 0.06637 | . 566 16.60 | 28.08 | 108.30 | 858.1 | 0.08455 | 0.10230 | 0.09251 | 0.05302 | 0.1590 | 0.05648 | 0.4564 | 1.0750 | 3.425 | 48.55 | 0.005903 | 0.03731 | 0.04730 | 0.01557 | 0.01318 | 0.003892 | 18.980 | 34.12 | 126.70 | 1124.0 | 0.11390 | 0.30940 | 0.3403 | 0.1418 | 0.2218 | 0.07820 | . 567 20.60 | 29.33 | 140.10 | 1265.0 | 0.11780 | 0.27700 | 0.35140 | 0.15200 | 0.2397 | 0.07016 | 0.7260 | 1.5950 | 5.772 | 86.22 | 0.006522 | 0.06158 | 0.07117 | 0.01664 | 0.02324 | 0.006185 | 25.740 | 39.42 | 184.60 | 1821.0 | 0.16500 | 0.86810 | 0.9387 | 0.2650 | 0.4087 | 0.12400 | . 568 7.76 | 24.54 | 47.92 | 181.0 | 0.05263 | 0.04362 | 0.00000 | 0.00000 | 0.1587 | 0.05884 | 0.3857 | 1.4280 | 2.548 | 19.15 | 0.007189 | 0.00466 | 0.00000 | 0.00000 | 0.02676 | 0.002783 | 9.456 | 30.37 | 59.16 | 268.6 | 0.08996 | 0.06444 | 0.0000 | 0.0000 | 0.2871 | 0.07039 | . 569 rows × 30 columns . features = [&#39;radius error&#39;,&#39;texture error&#39;,&#39;concave points error&#39;,&#39;symmetry error&#39;,&#39;worst texture&#39;,&#39;worst smoothness&#39;,&#39;worst symmetry&#39;] df = df[features] print(&#39;target[0:30]=&#39;,y[0:30]) df.head() . target[0:30]= [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0] . radius error texture error concave points error symmetry error worst texture worst smoothness worst symmetry . 0 1.0950 | 0.9053 | 0.01587 | 0.03003 | 17.33 | 0.1622 | 0.4601 | . 1 0.5435 | 0.7339 | 0.01340 | 0.01389 | 23.41 | 0.1238 | 0.2750 | . 2 0.7456 | 0.7869 | 0.02058 | 0.02250 | 25.53 | 0.1444 | 0.3613 | . 3 0.4956 | 1.1560 | 0.01867 | 0.05963 | 26.50 | 0.2098 | 0.6638 | . 4 0.7572 | 0.7813 | 0.01885 | 0.01756 | 16.67 | 0.1374 | 0.2364 | . splitting dataset into training and validation (85% - 15%) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(df,y,test_size=.15) . training and testing . cl = RandomForestClassifier(n_estimators=300) cl.fit(X_train,y_train) validation_e = cl.score(X_test,y_test) print(f&#39;{validation_e*100:.2f}% correct&#39;) . 95.35% correct . feature importance for classifiers . from rfpimp import * I = importances(cl,X_test,y_test) plot_importances(I,color=&#39;#4575b4&#39;,vscale=1.4) # therefore the pathologist should focus more on radius error . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; classifying handwritten digits . addr640 = pd.read_csv(&#39;640.csv&#39;) . print(addr640.digit.values) . [6 4 0] . addr640 = addr640.drop(&#39;digit&#39;,axis=1) . six_img_as_row = addr640.iloc[0].values img28x28 = six_img_as_row.reshape(28,28) print(type(six_img_as_row)) plt.imshow(img28x28,cmap=&#39;binary&#39;) plt.show() . &lt;class &#39;numpy.ndarray&#39;&gt; . 28x28 pixel image . six_img_as_row[six_img_as_row&gt;0] = 1 six_img_as_row = six_img_as_row.astype(int) img28x28 = six_img_as_row.reshape(28,28) s = str(img28x28).replace(&#39; &#39;,&#39;&#39;) print(s) . &lt;class &#39;numpy.ndarray&#39;&gt; [[0000000000000000000000000000] [0000000000000000000000000000] [0000000000000000000000000000] [0000000000000000000000000000] [0000000000000011111000000000] [0000000000000111101000000000] [0000000000001110000000000000] [0000000000011100000000000000] [0000000000011000000000000000] [0000000000110000000000000000] [0000000000100000000000000000] [0000000001100000000000000000] [0000000001100000000000000000] [0000000001100000000000000000] [0000000001100000000000000000] [0000000011000011111100000000] [0000000011000111111100000000] [0000000011101100000110000000] [0000000011101000000110000000] [0000000001100100000110000000] [0000000000100000000100000000] [0000000000110000000100000000] [0000000000011000001100000000] [0000000000000111111100000000] [0000000000000111111000000000] [0000000000000000000000000000] [0000000000000000000000000000] [0000000000000000000000000000]] . print(img28x28) print(six_img_as_row) . [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . load training data . digits =pd.read_csv(&#39;mnist-10k-sample.csv&#39;) images = digits.drop(&#39;digit&#39;,axis=1) targets = digits[&#39;digit&#39;] . fig, axes = plt.subplots(10,5,figsize=(4,6.5)) for i,ax in enumerate(axes.flat): img_as_row = images.iloc[i].values img28x28 = img_as_row.reshape(28,28) ax.axis(&#39;off&#39;) ax.imshow(img28x28,cmap=&#39;binary&#39;) ax.text(0,8,targets[i],color=&#39;#313695&#39;,fontsize=18) plt.show() . classifying test digits . from sklearn.ensemble import RandomForestClassifier cl = RandomForestClassifier(n_estimators=900,n_jobs=1) cl.fit(images,targets) pred = cl.predict(addr640) print(pred) . [6 7 0] . investigating error . import numpy as np np.set_printoptions(precision=3) # pb = [] for i in range(3): digit_values = range(10) prob = cl.predict_proba(addr640) prob_for_2nd_digit = prob[i] pred_digit = np.argmax(prob_for_2nd_digit) a = [prob_for_2nd_digit], &#39;Predicted digit is&#39;, pred_digit print(a) # pb.extend(a) # pb2.append(pred_digit) pred_digit = np.argmax(prob_for_2nd_digit) bars = plt.bar(digit_values,prob_for_2nd_digit,color=&#39;#4575b4&#39;) bars[pred_digit].set_color(&#39;#fdae61&#39;) plt.xlabel(&#39;predicted digit&#39;) plt.xticks(digit_values) plt.ylabel(&#39;likelihood 2nd image nis a specific digit&#39;) plt.show() # print(pb) . ([array([0.091, 0.044, 0.094, 0.051, 0.127, 0.166, 0.22 , 0.047, 0.09 , 0.07 ])], &#39;Predicted digit is&#39;, 6) . ([array([0.023, 0.138, 0.063, 0.139, 0.104, 0.084, 0.064, 0.249, 0.033, 0.101])], &#39;Predicted digit is&#39;, 7) . ([array([0.252, 0.032, 0.102, 0.088, 0.063, 0.137, 0.109, 0.147, 0.021, 0.049])], &#39;Predicted digit is&#39;, 0) . fours = images[targets==4] fig,axes = plt.subplots(15,8,figsize=(4,6.5)) for i,ax in enumerate(axes.flat): img = fours.iloc[i,:].values.reshape(28,28) ax.axis(&#39;off&#39;) ax.imshow(img,cmap=&#39;binary&#39;) . using a linear model for performance comparism . X_train,X_test,y_train,y_test = train_test_split(images,targets,test_size=.2) cl = RandomForestClassifier(n_estimators=900,n_jobs=-1) cl.fit(X_train,y_train) rfaccur = cl.score(X_test,y_test) print(rfaccur) . 0.9575 . from sklearn.linear_model import LogisticRegression # create linear model lm = LogisticRegression( solver=&#39;newton-cg&#39;,multi_class=&#39;multinomial&#39;) lm.fit(X_train,y_train) lmaccur =lm.score(X_test,y_test) print(lmaccur) . 0.91 . rf requires a lot of memory than the linear model . ntrees = cl.n_estimators nnodes = sum([cl.estimators_[i].tree_.node_count for i in range(ntrees)]) print(f&#39;{nnodes:,}&#39;) . 1,700,468 . import numpy as np X_train =np.array([1,2,3,4,5]).reshape(5,1) y_train =np.array([1,2,3,4,5]) X_test = np.array([6]).reshape(1,1) . linear regression model predicting target value 6 . from sklearn.linear_model import LinearRegression lm = LinearRegression() lm.fit(X_train,y_train) print(&#39;y =&#39;, lm.predict(X_test)) # linear relationship . y = [6.] . lm.fit(X_train,y_train).coef_ . array([1.]) . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(n_estimators=100) rf.fit(X_train,y_train) print(&#39;y =&#39;, rf.predict(X_test)) . y = [4.57] . !python prep-rent.py . Created rent.csv Created rent-ideal.csv .",
            "url": "https://d3la.github.io/wOL3/fastpages/jupyter/2020/12/03/cs229Part1.html",
            "relUrl": "/fastpages/jupyter/2020/12/03/cs229Part1.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "! pip install qeds . Collecting qeds Downloading https://files.pythonhosted.org/packages/41/50/509c79a019156862898acb9e23b28e872818c20492da187090167f6702ad/qeds-0.6.2.tar.gz Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from qeds) (1.0.5) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from qeds) (2.23.0) Collecting quandl Downloading https://files.pythonhosted.org/packages/1b/29/185269dbd2e2698c8098b35c52ce73a2c52cf76163e709f9f7789d03ebbb/Quandl-3.5.2-py2.py3-none-any.whl Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from qeds) (1.4.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from qeds) (1.18.5) Collecting quantecon Downloading https://files.pythonhosted.org/packages/5d/e3/4fd5f948de917036b5696347b28fa25da7bd7df995e4f9f42db1c3070eb8/quantecon-0.4.8-py3-none-any.whl (230kB) |████████████████████████████████| 235kB 6.0MB/s Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from qeds) (3.2.2) Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from qeds) (0.14.1) Requirement already satisfied: openpyxl in /usr/local/lib/python3.6/dist-packages (from qeds) (2.5.9) Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from qeds) (4.4.1) Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.6/dist-packages (from qeds) (0.8.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from qeds) (0.22.2.post1) Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from qeds) (0.10.1) Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from qeds) (0.10.2) Requirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;qeds) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;qeds) (2018.9) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;qeds) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;qeds) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;qeds) (2020.6.20) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;qeds) (3.0.4) Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl-&gt;qeds) (8.5.0) Collecting inflection&gt;=0.3.1 Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl-&gt;qeds) (1.15.0) Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from quantecon-&gt;qeds) (1.1.1) Requirement already satisfied: numba&gt;=0.38 in /usr/local/lib/python3.6/dist-packages (from quantecon-&gt;qeds) (0.48.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;qeds) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;qeds) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;qeds) (0.10.0) Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.6/dist-packages (from openpyxl-&gt;qeds) (1.0.1) Requirement already satisfied: jdcal in /usr/local/lib/python3.6/dist-packages (from openpyxl-&gt;qeds) (1.4.1) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly-&gt;qeds) (1.3.3) Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas_datareader-&gt;qeds) (4.2.6) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;qeds) (0.16.0) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels-&gt;qeds) (0.5.1) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy-&gt;quantecon-&gt;qeds) (1.1.0) Requirement already satisfied: llvmlite&lt;0.32.0,&gt;=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba&gt;=0.38-&gt;quantecon-&gt;qeds) (0.31.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba&gt;=0.38-&gt;quantecon-&gt;qeds) (50.3.0) Building wheels for collected packages: qeds Building wheel for qeds (setup.py) ... done Created wheel for qeds: filename=qeds-0.6.2-cp36-none-any.whl size=27821 sha256=7e13f5972afd7d6676eb9587e4cf0b29bcfdaf9b215dc4e26962f5e8ae8709df Stored in directory: /root/.cache/pip/wheels/b7/0b/74/c09109813c2b6116a2d4f2833c354b24163672f846a50fc7b4 Successfully built qeds Installing collected packages: inflection, quandl, quantecon, qeds Successfully installed inflection-0.5.1 qeds-0.6.2 quandl-3.5.2 quantecon-0.4.8 . import pandas as pd %matplotlib inline # activating plot theme import qeds qeds.themes.mpl_style . &lt;function qeds.themes.mpl_style&gt; . pd.__version__ . &#39;1.0.5&#39; . values = [5.6,5.3,4.3,4.2,5.8,5.3,4.6,7.8,9.1,8.,5.7] years = list(range(1995,2017,2)) unemp = pd.Series(data=values,index=years,name=&#39;Unemployment&#39;) unemp . 1995 5.6 1997 5.3 1999 4.3 2001 4.2 2003 5.8 2005 5.3 2007 4.6 2009 7.8 2011 9.1 2013 8.0 2015 5.7 Name: Unemployment, dtype: float64 . unemp.index . Int64Index([1995, 1997, 1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, 2015], dtype=&#39;int64&#39;) . unemp.values . array([5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8. , 5.7]) . unemp.name . &#39;Unemployment&#39; . unemp.plot() #python matplotlib plot command . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fadfba42550&gt; . unemp.unique() . array([5.6, 5.3, 4.3, 4.2, 5.8, 4.6, 7.8, 9.1, 8. , 5.7]) . unemp.plot.bar() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd33a57db00&gt; . data = { &quot;NorthEast&quot;: [5.9, 5.6, 4.4, 3.8, 5.8, 4.9, 4.3, 7.1, 8.3, 7.9, 5.7], &quot;MidWest&quot;: [4.5, 4.3, 3.6, 4. , 5.7, 5.7, 4.9, 8.1, 8.7, 7.4, 5.1], &quot;South&quot;: [5.3, 5.2, 4.2, 4. , 5.7, 5.2, 4.3, 7.6, 9.1, 7.4, 5.5], &quot;West&quot;: [6.6, 6., 5.2, 4.6, 6.5, 5.5, 4.5, 8.6, 10.7, 8.5, 6.1], &quot;National&quot;: [5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8., 5.7] } unemp_region = pd.DataFrame(data,index=years) unemp_region . NorthEast MidWest South West National . 1995 5.9 | 4.5 | 5.3 | 6.6 | 5.6 | . 1997 5.6 | 4.3 | 5.2 | 6.0 | 5.3 | . 1999 4.4 | 3.6 | 4.2 | 5.2 | 4.3 | . 2001 3.8 | 4.0 | 4.0 | 4.6 | 4.2 | . 2003 5.8 | 5.7 | 5.7 | 6.5 | 5.8 | . 2005 4.9 | 5.7 | 5.2 | 5.5 | 5.3 | . 2007 4.3 | 4.9 | 4.3 | 4.5 | 4.6 | . 2009 7.1 | 8.1 | 7.6 | 8.6 | 7.8 | . 2011 8.3 | 8.7 | 9.1 | 10.7 | 9.1 | . 2013 7.9 | 7.4 | 7.4 | 8.5 | 8.0 | . 2015 5.7 | 5.1 | 5.5 | 6.1 | 5.7 | . unemp_region[&#39;West&#39;]/100 . 1995 0.066 1997 0.060 1999 0.052 2001 0.046 2003 0.065 2005 0.055 2007 0.045 2009 0.086 2011 0.107 2013 0.085 2015 0.061 Name: West, dtype: float64 . unemp_region.corr() . NorthEast MidWest South West National . NorthEast 1.000000 | 0.875654 | 0.964415 | 0.967875 | 0.976016 | . MidWest 0.875654 | 1.000000 | 0.951379 | 0.900638 | 0.952389 | . South 0.964415 | 0.951379 | 1.000000 | 0.987259 | 0.995030 | . West 0.967875 | 0.900638 | 0.987259 | 1.000000 | 0.981308 | . National 0.976016 | 0.952389 | 0.995030 | 0.981308 | 1.000000 | . unemp_region.dtypes . NorthEast float64 MidWest float64 South float64 West float64 National float64 dtype: object . str_unemp = unemp_region.copy() str_unemp[&#39;South&#39;] = str_unemp[&#39;South&#39;].astype(str) str_unemp.dtypes . NorthEast float64 MidWest float64 South object West float64 National float64 dtype: object . str_unemp.head() . NorthEast MidWest South West National . 1995 5.9 | 4.5 | 5.3 | 6.6 | 5.6 | . 1997 5.6 | 4.3 | 5.2 | 6.0 | 5.3 | . 1999 4.4 | 3.6 | 4.2 | 5.2 | 4.3 | . 2001 3.8 | 4.0 | 4.0 | 4.6 | 4.2 | . 2003 5.8 | 5.7 | 5.7 | 6.5 | 5.8 | . str_unemp.sum() . NorthEast 63.7 MidWest 62 South 5.35.24.24.05.75.24.37.69.17.45.5 West 72.8 National 65.7 dtype: object . unemp_region[&quot;UnweightedMean&quot;] = (unemp_region[&quot;NorthEast&quot;] + unemp_region[&quot;MidWest&quot;] + unemp_region[&quot;South&quot;] + unemp_region[&quot;West&quot;])/4 . unemp_region.tail() . NorthEast MidWest South West National UnweightedMean . 2007 4.3 | 4.9 | 4.3 | 4.5 | 4.6 | 4.50 | . 2009 7.1 | 8.1 | 7.6 | 8.6 | 7.8 | 7.85 | . 2011 8.3 | 8.7 | 9.1 | 10.7 | 9.1 | 9.20 | . 2013 7.9 | 7.4 | 7.4 | 8.5 | 8.0 | 7.80 | . 2015 5.7 | 5.1 | 5.5 | 6.1 | 5.7 | 5.60 | . names = {&#39;NorthEast&#39;:&#39;NE&#39;,&#39;MidWest&#39;:&#39;MW&#39;,&#39;South&#39;:&#39;S&#39;,&#39;West&#39;:&#39;W&#39;} unemp_region.rename(columns=names,inplace=True) unemp_region.head() . NE MW S W National UnweightedMean . 1995 5.9 | 4.5 | 5.3 | 6.6 | 5.6 | 5.575 | . 1997 5.6 | 4.3 | 5.2 | 6.0 | 5.3 | 5.275 | . 1999 4.4 | 3.6 | 4.2 | 5.2 | 4.3 | 4.350 | . 2001 3.8 | 4.0 | 4.0 | 4.6 | 4.2 | 4.100 | . 2003 5.8 | 5.7 | 5.7 | 6.5 | 5.8 | 5.925 | . # loading up data url = &quot;https://datascience.quantecon.org/assets/data/state_unemployment.csv&quot; unemp_raw = pd.read_csv(url,parse_dates=[&#39;Date&#39;]) # load up the Date column as a Python datetime type unemp_raw.head() . Date state LaborForce UnemploymentRate . 0 2000-01-01 | Alabama | 2142945.0 | 4.7 | . 1 2000-01-01 | Alaska | 319059.0 | 6.3 | . 2 2000-01-01 | Arizona | 2499980.0 | 4.1 | . 3 2000-01-01 | Arkansas | 1264619.0 | 4.4 | . 4 2000-01-01 | California | 16680246.0 | 5.0 | . unemp_raw.shape . (10800, 4) . # data transformation unemp_all = (unemp_raw .reset_index() .pivot_table(index=&#39;Date&#39;,columns=&#39;state&#39;,values=&#39;UnemploymentRate&#39;) ) unemp_all.head() . state Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New Hampshire New Mexico New York New jersey North Carolina North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode island South Carolina South Dakota Tennessee Texas Utah Vermont Virginia Washington West Virginia Wisconsin Wyoming . Date . 2000-01-01 4.7 | 6.3 | 4.1 | 4.4 | 5.0 | 2.8 | 2.8 | 3.5 | 3.7 | 3.7 | 4.7 | 4.6 | 4.2 | 3.2 | 2.4 | 3.4 | 4.2 | 5.1 | 3.6 | 3.4 | 3.0 | 3.3 | 3.0 | 5.4 | 3.1 | 5.1 | 2.8 | 3.9 | 2.7 | 5.1 | 4.7 | 3.8 | 3.3 | 3.1 | 4.1 | 3.2 | 5.0 | 4.1 | 4.0 | 4.0 | 2.4 | 3.7 | 4.6 | 3.1 | 2.7 | 2.6 | 4.9 | 5.8 | 3.2 | 4.1 | . 2000-02-01 4.7 | 6.3 | 4.1 | 4.3 | 5.0 | 2.8 | 2.7 | 3.6 | 3.7 | 3.6 | 4.6 | 4.6 | 4.2 | 3.3 | 2.4 | 3.4 | 4.1 | 5.1 | 3.5 | 3.4 | 2.9 | 3.2 | 3.0 | 5.5 | 3.2 | 5.0 | 2.8 | 3.9 | 2.7 | 5.0 | 4.7 | 3.7 | 3.3 | 3.0 | 4.1 | 3.1 | 5.0 | 4.0 | 4.0 | 3.9 | 2.4 | 3.7 | 4.6 | 3.1 | 2.6 | 2.5 | 4.9 | 5.6 | 3.2 | 3.9 | . 2000-03-01 4.6 | 6.3 | 4.0 | 4.3 | 5.0 | 2.7 | 2.6 | 3.6 | 3.7 | 3.6 | 4.5 | 4.6 | 4.3 | 3.3 | 2.4 | 3.5 | 4.1 | 5.1 | 3.5 | 3.5 | 2.8 | 3.2 | 3.0 | 5.5 | 3.3 | 5.0 | 2.8 | 4.0 | 2.7 | 4.9 | 4.6 | 3.6 | 3.4 | 3.0 | 4.0 | 3.1 | 5.0 | 4.0 | 4.0 | 3.8 | 2.4 | 3.8 | 4.5 | 3.1 | 2.6 | 2.4 | 5.0 | 5.5 | 3.3 | 3.9 | . 2000-04-01 4.6 | 6.3 | 4.0 | 4.3 | 5.1 | 2.7 | 2.5 | 3.7 | 3.7 | 3.7 | 4.4 | 4.6 | 4.3 | 3.3 | 2.5 | 3.5 | 4.1 | 5.1 | 3.4 | 3.5 | 2.7 | 3.3 | 3.0 | 5.6 | 3.3 | 5.0 | 2.8 | 4.0 | 2.7 | 4.9 | 4.6 | 3.6 | 3.4 | 2.9 | 4.0 | 3.1 | 5.0 | 4.0 | 4.0 | 3.7 | 2.4 | 3.8 | 4.4 | 3.1 | 2.7 | 2.4 | 5.0 | 5.4 | 3.4 | 3.8 | . 2000-05-01 4.5 | 6.3 | 4.0 | 4.2 | 5.1 | 2.7 | 2.4 | 3.7 | 3.7 | 3.7 | 4.3 | 4.6 | 4.3 | 3.3 | 2.5 | 3.5 | 4.1 | 5.1 | 3.3 | 3.6 | 2.6 | 3.5 | 3.0 | 5.6 | 3.3 | 5.0 | 2.8 | 4.0 | 2.7 | 4.9 | 4.6 | 3.6 | 3.5 | 2.9 | 4.1 | 3.1 | 5.1 | 4.0 | 4.1 | 3.8 | 2.4 | 3.9 | 4.3 | 3.2 | 2.7 | 2.3 | 5.1 | 5.4 | 3.5 | 3.8 | . states = [ &#39;Arizona&#39;,&#39;California&#39;,&#39;Florida&#39;,&#39;Illinois&#39;, &#39;Michigan&#39;,&#39;New York&#39;,&#39;Texas&#39; ] unemp = unemp_all[states] unemp.head() # a subset . state Arizona California Florida Illinois Michigan New York Texas . Date . 2000-01-01 4.1 | 5.0 | 3.7 | 4.2 | 3.3 | 4.7 | 4.6 | . 2000-02-01 4.1 | 5.0 | 3.7 | 4.2 | 3.2 | 4.7 | 4.6 | . 2000-03-01 4.0 | 5.0 | 3.7 | 4.3 | 3.2 | 4.6 | 4.5 | . 2000-04-01 4.0 | 5.1 | 3.7 | 4.3 | 3.3 | 4.6 | 4.4 | . 2000-05-01 4.0 | 5.1 | 3.7 | 4.3 | 3.5 | 4.6 | 4.3 | . unemp.plot(figsize=(8,6)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd32f50e400&gt; . unemp.index . DatetimeIndex([&#39;2000-01-01&#39;, &#39;2000-02-01&#39;, &#39;2000-03-01&#39;, &#39;2000-04-01&#39;, &#39;2000-05-01&#39;, &#39;2000-06-01&#39;, &#39;2000-07-01&#39;, &#39;2000-08-01&#39;, &#39;2000-09-01&#39;, &#39;2000-10-01&#39;, ... &#39;2017-03-01&#39;, &#39;2017-04-01&#39;, &#39;2017-05-01&#39;, &#39;2017-06-01&#39;, &#39;2017-07-01&#39;, &#39;2017-08-01&#39;, &#39;2017-09-01&#39;, &#39;2017-10-01&#39;, &#39;2017-11-01&#39;, &#39;2017-12-01&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, length=216, freq=None) . unemp.loc[&#39;01/01/2000&#39;] . state Arizona 4.1 California 5.0 Florida 3.7 Illinois 4.2 Michigan 3.3 New York 4.7 Texas 4.6 Name: 2000-01-01 00:00:00, dtype: float64 . unemp.loc[&#39;01/01/2000&#39;:&#39;06/01/2000&#39;] . state Arizona California Florida Illinois Michigan New York Texas . Date . 2000-01-01 4.1 | 5.0 | 3.7 | 4.2 | 3.3 | 4.7 | 4.6 | . 2000-02-01 4.1 | 5.0 | 3.7 | 4.2 | 3.2 | 4.7 | 4.6 | . 2000-03-01 4.0 | 5.0 | 3.7 | 4.3 | 3.2 | 4.6 | 4.5 | . 2000-04-01 4.0 | 5.1 | 3.7 | 4.3 | 3.3 | 4.6 | 4.4 | . 2000-05-01 4.0 | 5.1 | 3.7 | 4.3 | 3.5 | 4.6 | 4.3 | . 2000-06-01 4.0 | 5.1 | 3.8 | 4.3 | 3.7 | 4.6 | 4.3 | . # Data aggregations are used extensively to analyze and manipulate data unemp.mean() . state Arizona 6.301389 California 7.299074 Florida 6.048611 Illinois 6.822685 Michigan 7.492593 New York 6.102315 Texas 5.695370 dtype: float64 . unemp.var(axis=1) . Date 2000-01-01 0.352381 2000-02-01 0.384762 2000-03-01 0.364762 2000-04-01 0.353333 2000-05-01 0.294762 ... 2017-08-01 0.141429 2017-09-01 0.163333 2017-10-01 0.165714 2017-11-01 0.165714 2017-12-01 0.148095 Length: 216, dtype: float64 . #classify states as “low unemployment” or “high unemployment” based on whether their mean unemployment level is above or below 6.5 def high_or_low(s): &quot;&quot;&quot; high if the mean is above 6.5 and low if the mean is below 6.5 &quot;&quot;&quot; if s.mean() &lt; 6.5: out =&#39;Low&#39; else: out = &#39;High&#39; return out . unemp.agg(high_or_low) . state Arizona Low California High Florida Low Illinois High Michigan High New York Low Texas Low dtype: object . unemp.agg(high_or_low,axis=1) . Date 2000-01-01 Low 2000-02-01 Low 2000-03-01 Low 2000-04-01 Low 2000-05-01 Low ... 2017-08-01 Low 2017-09-01 Low 2017-10-01 Low 2017-11-01 Low 2017-12-01 Low Length: 216, dtype: object . unemp.agg([min,max,high_or_low]) . Arizona California Florida Illinois Michigan New York Texas . min 3.6 | 4.5 | 3.1 | 4.2 | 3.2 | 4.2 | 3.9 | . max 10.9 | 12.3 | 11.3 | 11.3 | 14.6 | 8.9 | 8.3 | . high_or_low Low | High | Low | High | High | Low | Low | . unemp.agg([min,max,high_or_low]).columns . Index([&#39;Arizona&#39;, &#39;California&#39;, &#39;Florida&#39;, &#39;Illinois&#39;, &#39;Michigan&#39;, &#39;New York&#39;, &#39;Texas&#39;], dtype=&#39;object&#39;) . .",
            "url": "https://d3la.github.io/wOL3/2020/12/03/QuantEconDS.html",
            "relUrl": "/2020/12/03/QuantEconDS.html",
            "date": " • Dec 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Love&amp;Compassion; Truth&amp;Conviction .",
          "url": "https://d3la.github.io/wOL3/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://d3la.github.io/wOL3/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}